"""Modulos necesarios para el scraper de libros."""
import time  # Para manejar esperas y temporizaciones
from urllib.parse import urljoin  # Para construir URLs absolutas
from datetime import datetime  # Para generar nombres de archivo con timestamp
import csv  # Para guardar los datos en formato CSV
from bs4 import BeautifulSoup  # Para analizar y extraer contenido HTML
import requests  # Para realizar peticiones HTTP
import schedule  # Para programar tareas autom√°ticas

# URL base del sitio a scrapear
BASE_URL = "http://books.toscrape.com/catalogue/"

# Encabezado personalizado para simular un navegador real
HEADER = {"User-Agent": "Mozilla/5.0"}

def obtener_html(url):
    """
    Realiza una solicitud HTTP a la URL proporcionada y retorna el HTML.
    
    Par√°metros:
        url (str): La URL de la p√°gina a obtener.
    
    Retorna:
        str: El contenido HTML de la p√°gina, o None si ocurre un error.
    """
    try:
        response = requests.get(url, headers=HEADER, timeout=10)  # Hace la petici√≥n HTTP
        response.raise_for_status()  # Lanza error si el estado no es 200 (OK)
        return response.text  # Devuelve el contenido HTML
    except requests.RequestException as e:
        print(f"‚ùå Error al obtener la p√°gina {url}: {e}")  # Muestra error
        return None  # Devuelve None si hay fallo

def parsear_pagina(html):
    """
    Extrae los datos de los libros presentes en la p√°gina HTML.
    
    Par√°metros:
        html (str): Contenido HTML de una p√°gina de libros.
    
    Retorna:
        tuple: (lista de libros, bot√≥n 'next' si existe)
    """
    soup = BeautifulSoup(html, "html.parser")  # Convierte HTML a objeto navegable
    libros = soup.find_all("article", class_="product_pod")  # Encuentra todos los libros en la p√°gina
    
    datos = []
    for libro in libros:
        titulo = libro.h3.a["title"]  # Obtiene el t√≠tulo del libro
        precio = libro.find("p", class_="price_color").text  # Precio
        disponibilidad = libro.find("p", class_="instock availability").text.strip()  # Disponibilidad (con espacios limpiados)
        calificacion = libro.p["class"][1] if len(libro.p["class"]) > 1 else "SinCalificaci√≥n"  # Calificaci√≥n por clase CSS
        enlace_relativo = libro.h3.a["href"]  # Enlace relativo al libro
        enlace_completo = urljoin(BASE_URL, enlace_relativo)  # Enlace absoluto
        
        datos.append({
            "T√≠tulo": titulo,
            "Precio": precio,
            "Disponibilidad": disponibilidad,
            "Calificaci√≥n": calificacion,
            "Enlace": enlace_completo
        })
    
    return datos, soup.find("li", class_="next")  # Devuelve los datos y el bot√≥n "next" si existe


def ejecutar_scraper():
    """
    Ejecuta el scraping sobre todas las p√°ginas del cat√°logo
    y guarda los datos en un archivo CSV.
    """
    pagina_actual = "page-1.html"  # Comienza desde la primera p√°gina
    todos_los_datos = []  # Lista para acumular todos los libros
    
    while pagina_actual:
        url = urljoin(BASE_URL, pagina_actual)  # Construye URL completa
        print(f"üìÑ Procesando: {url}")
        html = obtener_html(url)  # Obtiene HTML de la p√°gina actual
        if not html:
            break  # Si falla la petici√≥n, termina
        
        datos, next_btn = parsear_pagina(html)  # Extrae libros y el bot√≥n "next"
        todos_los_datos.extend(datos)  # Agrega los libros encontrados a la lista
        
        if next_btn:
            pagina_actual = next_btn.a["href"]  # Actualiza con la siguiente p√°gina
        else:
            pagina_actual = None  # Fin del cat√°logo
        
        time.sleep(1)  # Espera 1 segundo para evitar saturar el servidor
    
    guardar_csv(todos_los_datos)  # Guarda todos los libros en un archivo CSV

def guardar_csv(datos):
    """
    Guarda la lista de libros en un archivo CSV con nombre √∫nico por timestamp.
    
    Par√°metros:
        datos (list): Lista de diccionarios con los datos de los libros.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  # Fecha actual para nombrar el archivo
    nombre_archivo = f"libros_{timestamp}.csv"  # Nombre del archivo con fecha
    
    with open(nombre_archivo, "w", newline="", encoding="utf-8") as archivo:
        campos = ["T√≠tulo", "Precio", "Disponibilidad", "Calificaci√≥n", "Enlace"]
        writer = csv.DictWriter(archivo, fieldnames=campos)  # Prepara el escritor CSV
        writer.writeheader()  # Escribe la cabecera (nombres de columnas)
        writer.writerows(datos)  # Escribe los datos de cada libro
    
    print(f"‚úÖ Datos guardados en '{nombre_archivo}'.")

def tarea_programada():
    """
    Funci√≥n envoltorio que se ejecuta autom√°ticamente seg√∫n la programaci√≥n.
    """
    print(f"üïí Ejecutando scraper: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    ejecutar_scraper()  # Ejecuta todo el scraping

schedule.every(1).hours.do(tarea_programada)  # Programa para ejecutar scraper cada 1 hora

if __name__ == "__main__":  # Solo se ejecuta si este archivo es el principal
    print("üîÅ Iniciando programa en modo autom√°tico. Presiona Ctrl+C para detener.")
    try:
        while True:
            schedule.run_pending()  # Ejecuta las tareas programadas si corresponde
            time.sleep(1)  # Espera 1 segundo antes de volver a comprobar
    except KeyboardInterrupt:
        print("\nüõë Programa detenido por el usuario.")